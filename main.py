from transformers import AutoModelForCausalLM, AutoTokenizer
import os

MODELS = {
    "1": {"name": "gpt2", "size": "500MB"},
    "2": {"name": "facebook/opt-1.3b", "size": "2.7GB"},
    "3": {"name": "mistralai/Mistral-7B-v0.1", "size": "14GB"}
}

def download_model(model_name):
    print(f"‚è≥ –°–∫–∞—á–∏–≤–∞—é {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    return model, tokenizer

def chat_loop(model, tokenizer):
    print("\nüí¨ –ù–∞—á–∏–Ω–∞–µ–º –¥–∏–∞–ª–æ–≥ (–¥–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ 'exit')")
    while True:
        input_text = input("–¢—ã: ")
        if input_text.lower() == 'exit':
            break
        
        inputs = tokenizer(input_text, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=50)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"–ë–æ—Ç: {response[len(input_text):]}")

if __name__ == "__main__":
    print("üõ† –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
    for key, value in MODELS.items():
        print(f"{key}. {value['name']} ({value['size']})")
    
    choice = input("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (1-3): ")
    if choice in MODELS:
        model, tokenizer = download_model(MODELS[choice]["name"])
        chat_loop(model, tokenizer)
    else:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
